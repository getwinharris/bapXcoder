# qwen3VL-Local-CLI Configuration

# Model settings
[model]
# Path to your Qwen3VL GGUF model file
model_path = Qwen3VL-8B-Instruct-Q8_0.gguf
# Model download URL (for reference)
model_url = https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct-GGUF/resolve/main/Qwen3VL-8B-Instruct-Q8_0.gguf?download=true

# Runtime settings
[defaults]
# Default max tokens to generate
max_tokens = 512

# Default temperature (0.0 to 1.0, where 0.0 is deterministic and 1.0 is more random)
temperature = 0.7

# Number of CPU threads to use
threads = 4

# Context size (how much history to keep)
context_size = 4096

# GPU settings (set to 0 for CPU only, >0 for GPU layers)
gpu_layers = 0

# Server settings
[server]
# Host address to run the web interface
host = 127.0.0.1

# Port to run the web interface
port = 7860