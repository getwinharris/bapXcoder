# qwen3VL-Local-CLI Configuration

# Model settings
[model]
# Path to your Qwen3VL GGUF model file
model_path = Qwen3VL-8B-Instruct-Q8_0.gguf

# Runtime settings
[defaults]
# Default max tokens to generate
max_tokens = 512

# Default temperature (0.0 to 1.0, where 0.0 is deterministic and 1.0 is more random)
temperature = 0.7

# Number of CPU threads to use
threads = 4

# Context size (how much history to keep)
context_size = 4096

# GPU settings (set to 0 for CPU only, >0 for GPU layers)
gpu_layers = 0