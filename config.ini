# bapXcoder Configuration

# Model settings
[model]
# Path to your bapXcoder GGUF model file
model_path = Qwen2.5-Coder-7B-Instruct-Q8_0.gguf
# Model access URL (for runtime connection)
model_url = https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/Qwen2.5-Coder-7B-Instruct-Q8_0.gguf
# Model selection: Single unified model for all tasks (like Trae.ai SOLO)
model_type = unified

# Runtime settings
[defaults]
# Default max tokens to generate
max_tokens = 1024

# Default temperature (0.0 to 1.0, where 0.0 is deterministic and 1.0 is more random)
temperature = 0.7

# Number of CPU threads to use
threads = 4

# Context size (how much history to keep)
context_size = 8192

# GPU settings (set to 0 for CPU only, >0 for GPU layers)
gpu_layers = 0

# Server settings
[server]
# Host address to run the web interface
host = 127.0.0.1

# Port to run the web interface
port = 7860

# Advanced settings
[advanced]
# Enable automatic model download if not found
auto_download_model = true
# Single model handles all tasks (coding, planning, analysis)
unified_model = true
# Multi-agent simulation for different tasks
multi_agent_simulation = true